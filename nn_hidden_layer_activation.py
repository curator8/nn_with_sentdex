# activation functions review

# given the ouput into neuron will determine output of neuron

# sigmoid functions was a little more easy to train a neural network
# more granular output 
# this help when training
# suffers from vanishing gradient problem

# rectify linear (reLU) function is preferable 
# fast and granular and less complicated  

# the reason why we don't use linear function is 
# because we can only fit linear functions 
# no way of fitting non linear data 

# when training nueral network 
# both neuron must be activated see affect on neural network


